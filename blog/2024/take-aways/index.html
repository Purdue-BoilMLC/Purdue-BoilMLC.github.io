<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Advancements in Language Models and Generative Adversarial Networks | Boilermakers Machine Learning Collaborative (Boil-MLC) | Students Machine Learning Seminar at Purdue University </title> <meta name="author" content=" "> <meta name="description" content="A paper review on WGAN &amp; Gopher"> <meta name="keywords" content="Machine Learning, Interdisciplinary Collaboration, Academic Initiative, PhD Students, Research, Innovation, Workshops, Seminars, Purdue University, Computer Science, Mathematics, Statistics"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A5%9B&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://purdue-boilmlc.github.io/blog/2024/take-aways/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Advancements in Language Models and Generative Adversarial Networks",
            "description": "A paper review on WGAN & Gopher",
            "published": "May 29, 2024",
            "authors": [
              
              {
                "author": "Yuetian Chen",
                "authorURL": "https://stry233.github.io/",
                "affiliations": [
                  {
                    "name": "TruSeLab, Purdue University",
                    "url": "https://www.cs.purdue.edu/truselab/"
                  }
                ]
              },
              
              {
                "author": "Yineng Chen",
                "authorURL": "https://chernyn.github.io/",
                "affiliations": [
                  {
                    "name": "Purdue University",
                    "url": "https://www.purdue.edu"
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Boil-MLC </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Articles </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Events </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">People </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Advancements in Language Models and Generative Adversarial Networks</h1> <p>A paper review on WGAN &amp; Gopher</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#introduction">Introduction</a> </div> <div> <a href="#scaling-language-models">Scaling Language Models</a> </div> <ul> <li> <a href="#the-gopher-architecture">The Gopher Architecture</a> </li> <li> <a href="#key-insights-from-gopher">Key Insights from Gopher</a> </li> </ul> <div> <a href="#wasserstein-gans">Wasserstein GANs</a> </div> <ul> <li> <a href="#overcoming-challenges-in-standard-gans">Overcoming Challenges in Standard GANs</a> </li> <li> <a href="#the-wasserstein-distance">The Wasserstein Distance</a> </li> </ul> <div> <a href="#conclusion">Conclusion</a> </div> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <p>Recent years have seen tremendous progress in the fields of natural language processing (NLP) and generative modeling. Language models like GPT-3 <a class="citation" href="#brown2020language">(Brown et al., 2020)</a> and PaLM <a class="citation" href="#chowdhery2022palm">(Chowdhery et al., 2022)</a> have demonstrated remarkable capabilities in understanding and generating human-like text. Meanwhile, generative adversarial networks (GANs) have enabled the creation of strikingly realistic images, videos, and other media.</p> <p>In this blog post, we dive into two significant papers that have advanced these domains: <span id="rae2022scaling"><i>Scaling Language Models: Methods, Analysis &amp; Insights from Training Gopher</i>. (2022).</span> and <span id="arjovsky2017wasserstein">Arjovsky, M., Chintala, S., &amp; Bottou, L. (2017). <i>Wasserstein GAN</i>.</span>. We discuss the key ideas, methodologies, and implications of these works.</p> <h2 id="scaling-language-models">Scaling Language Models</h2> <p>Language models form the backbone of modern NLP systems. By learning from vast corpora of text data, these models acquire a broad understanding of language that can be applied to diverse downstream tasks. The Gopher model <a class="citation" href="#rae2022scaling">(<i>Scaling Language Models: Methods, Analysis &amp; Insights from Training Gopher</i>, 2022)</a>, developed by DeepMind, represents an important milestone in the scaling of language models.</p> <h3 id="the-gopher-architecture">The Gopher Architecture</h3> <p>Gopher is a large-scale transformer-based language model. It follows the decoder-only architecture that has become standard for models like GPT-3. However, the Gopher architecture incorporates a few notable modifications:</p> <ol> <li> <p><strong>RMSNorm instead of LayerNorm</strong>: Gopher uses RMSNorm <a class="citation" href="#zhang2019root">(Zhang &amp; Sennrich, 2019)</a> in place of the more common LayerNorm. RMSNorm is computationally simpler and may promote more stable gradients in deep networks.</p> </li> <li> <p><strong>Relative Positional Encodings</strong>: Instead of absolute positional encodings, Gopher employs relative encodings. This allows the model to better capture the relative distances between tokens in the input sequence.</p> </li> </ol> <h3 id="key-insights-from-gopher">Key Insights from Gopher</h3> <p>The Gopher paper provides several valuable insights into the behavior of large language models:</p> <blockquote> <p>The benefits of scale are most pronounced on tasks like reading comprehension, fact-checking, and toxicity detection. Logical and mathematical reasoning see more modest improvements.</p> <cite><a class="citation" href="#rae2022scaling">(<i>Scaling Language Models: Methods, Analysis &amp; Insights from Training Gopher</i>, 2022)</a></cite> </blockquote> <p>This discrepancy may be attributed to the distribution of the training data, underscoring the importance of dataset curation.</p> <p>Overall, Gopher demonstrates the immense potential of scaling up language models in terms of both parameter count and data size.</p> <h2 id="wasserstein-gans">Wasserstein GANs</h2> <p>GANs <a class="citation" href="#goodfellow2014generative">(Goodfellow et al., 2014)</a> have emerged as a powerful framework for generative modeling, especially in the image domain. However, training GANs is notoriously challenging, with issues like mode collapse and unstable dynamics plaguing many variants. The Wasserstein GAN (WGAN) <a class="citation" href="#arjovsky2017wasserstein">(Arjovsky et al., 2017)</a> introduces a novel approach that mitigates these problems.</p> <h3 id="overcoming-challenges-in-standard-gans">Overcoming Challenges in Standard GANs</h3> <p>In a standard GAN, the generator and discriminator are trained in an adversarial game. The generator tries to produce samples that fool the discriminator, while the discriminator learns to distinguish real from generated data. This setup can lead to several pathologies <span id="arjovsky2017wasserstein">Arjovsky, M., Chintala, S., &amp; Bottou, L. (2017). <i>Wasserstein GAN</i>.</span>:</p> <ul> <li>If the discriminator becomes too strong, it may perfectly reject all generator samples, leading to vanishing gradients and stalled training.</li> <li>The generator may collapse onto a narrow distribution, sacrificing diversity in an effort to exploit the discriminator.</li> </ul> <p>WGAN proposes a principled solution to these challenges, rooted in the theory of optimal transport.</p> <h3 id="the-wasserstein-distance">The Wasserstein Distance</h3> <p>The Wasserstein distance, also known as the Earth Mover’s distance, is a metric that quantifies the dissimilarity between two probability distributions. In the context of GANs, it measures the distance between the generated distribution $P_g$ and the real data distribution $P_r$. The Wasserstein distance is defined as:</p> \[\label{eq:wasserstein} W(P_r, P_g) = \inf_{\gamma \in \Pi(P_r, P_g)} \mathbb{E}_{(x, y) \sim \gamma}[\|x - y\|]\] <p>where $\Pi(P_r, P_g)$ denotes the set of all joint distributions $\gamma(x, y)$ whose marginals are $P_r$ and $P_g$, respectively. In other words, the Wasserstein distance is the minimum cost of transporting mass from $P_r$ to $P_g$, where the cost is given by the expectation of the Euclidean distance between points.</p> <p>The key advantage of the Wasserstein distance is that it is continuous and differentiable almost everywhere, even when the supports of $P_r$ and $P_g$ do not overlap. This is in contrast to other commonly used metrics like the Jensen-Shannon divergence, which can lead to vanishing gradients in such scenarios <a class="citation" href="#arjovsky2017wasserstein">(Arjovsky et al., 2017)</a>.</p> <p>To make the Wasserstein distance tractable to compute, the Kantorovich-Rubinstein duality <a class="citation" href="#villani2009optimal">(Villani &amp; others, 2009)</a> is employed:</p> \[\label{eq:kantorovich} W(P_r, P_g) = \sup_{\|f\|_L \leq 1} \mathbb{E}_{x \sim P_r}[f(x)] - \mathbb{E}_{x \sim P_g}[f(x)]\] <p>where the supremum is taken over all 1-Lipschitz functions $f$. In practice, the discriminator in WGAN is trained to approximate the optimal $f$, while the generator is trained to minimize the Wasserstein distance.</p> <p>The Lipschitz constraint on the discriminator is enforced through gradient penalty <a class="citation" href="#gulrajani2017improved">(Gulrajani et al., 2017)</a>:</p> \[\label{eq:gradient_penalty} \mathcal{L}_{\text{GP}} = \lambda \mathbb{E}_{\hat{x} \sim P_{\hat{x}}}[(\|\nabla_{\hat{x}} D(\hat{x})\|_2 - 1)^2]\] <p>where $P_{\hat{x}}$ is the distribution of points along straight lines between pairs of points sampled from $P_r$ and $P_g$. This gradient penalty term is added to the discriminator loss to encourage 1-Lipschitzness.</p> <p>By minimizing the Wasserstein distance through this dual formulation, WGAN achieves more stable training and generates higher quality samples compared to standard GANs. The theoretical grounding in optimal transport theory provides a principled foundation for the WGAN framework.</p> <h2 id="conclusion">Conclusion</h2> <p>The Gopher and WGAN papers exemplify the rapid progress happening in language modeling and generative AI. As we scale up our models and refine our training objectives, we unlock new frontiers in machine intelligence. Gopher points towards a future where language models serve as flexible, general-purpose tools for a wide range of applications. WGAN, meanwhile, brings us closer to generating high-fidelity content with deep neural networks.</p> <p>Moving forward, it will be exciting to see how these ideas are extended and combined. The union of powerful language models and advanced generative techniques could lead to transformative breakthroughs, reshaping how we interact with AI systems. As always, however, it is crucial that we approach these developments thoughtfully, considering the broader societal implications alongside the technical innovations.</p> <h2>References</h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="brown2020language" class="col-sm-8"> <div class="title">Language Models are Few-Shot Learners</div> <div class="author"> Tom B. Brown, Benjamin Mann, Nick Ryder, and <span class="more-authors" title="click to view 28 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '28 more authors' ? 'Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei' : '28 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">28 more authors</span> </div> <div class="periodical"> 2020 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="chowdhery2022palm" class="col-sm-8"> <div class="title">PaLM: Scaling Language Modeling with Pathways</div> <div class="author"> Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, and <span class="more-authors" title="click to view 64 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '64 more authors' ? 'Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, Noah Fiedel' : '64 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">64 more authors</span> </div> <div class="periodical"> 2022 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="villani2009optimal" class="col-sm-8"> <div class="title">Optimal transport: old and new</div> <div class="author"> Cédric Villani, and  others </div> <div class="periodical"> 2009 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="gulrajani2017improved" class="col-sm-8"> <div class="title">Improved Training of Wasserstein GANs</div> <div class="author"> Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Vincent Dumoulin, Aaron Courville' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> 2017 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="rae2022scaling" class="col-sm-8"> <div class="title">Scaling Language Models: Methods, Analysis &amp; Insights from Training Gopher</div> <div class="author"> </div> <div class="periodical"> 2022 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="arjovsky2017wasserstein" class="col-sm-8"> <div class="title">Wasserstein GAN</div> <div class="author"> Martin Arjovsky, Soumith Chintala, and Léon Bottou </div> <div class="periodical"> 2017 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="zhang2019root" class="col-sm-8"> <div class="title">Root Mean Square Layer Normalization</div> <div class="author"> Biao Zhang, and Rico Sennrich </div> <div class="periodical"> 2019 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="goodfellow2014generative" class="col-sm-8"> <div class="title">Generative Adversarial Networks</div> <div class="author"> Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, and <span class="more-authors" title="click to view 5 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '5 more authors' ? 'Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio' : '5 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">5 more authors</span> </div> <div class="periodical"> 2014 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> </ol> </div> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2024-05-29-review.bib"></d-bibliography> <div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"Purdue-BoilMLC/Purdue-BoilMLC.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2024 . Made with ❤ by <a href="https://Purdue-BoilMLC.github.io">Boil-MLC</a>. The source code is licensed <a href="https://github.com/Purdue-BoilMLC/Purdue-BoilMLC.github.io/blob/master/LICENSE" rel="external nofollow noopener" target="_blank">MIT</a>. Last updated: June 18, 2024. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>