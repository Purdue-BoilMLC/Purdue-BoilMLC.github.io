---
layout: distill
title: "II. Introduction to Rare Event Simulation"
tags: Rare-Event-Simulation
categories: lecture-notes
giscus_comments: true
date: 2024-07-15 09:00:00-0400
featured: false
# thumbnail: assets/img/blog/2024-07-10-mist/cover.jpeg

authors:
  - name: Yineng Chen
    url: "https://chernyn.github.io/"
    affiliations:
      name: Purdue University
      url: https://www.purdue.edu

bibliography: 2024-07-10-mist.bib

toc:
  - name: Importance Sampling (IS)
  - name: An Asymptotic Rare Event Setting


---

## Importance Sampling (IS)

Considering the bad behavior of the simplest estimator in the last note, We try to change the ditribution give a new estimator:

$$
\widetilde{f_M}=\frac{1}{M}\sum_{j=1}^Mf(Y^{(j)})\frac{\pi(Y^{(j)})}{\widetilde{\pi}(Y^{(j)})}
$$

This estimator is __unbiased__, in the sense that

$$
\mathbb{E}[\widetilde{f_M}]=\mathbb{E}_{\pi}[f(Y)\frac{\pi(Y)}{\widetilde{\pi}(Y)}]=\int f(y)\frac{\pi(y)}{\widetilde{\pi}(y)}\widetilde{\pi}(y)dy=\pi[f]
$$

In this way, we have

$$
\begin{equation*}
\begin{aligned}
\text{var}(\widetilde{f_M})&=\frac{1}{M}\text{var}_{\widetilde{\pi}}(f(Y)\frac{\pi(Y)}{\widetilde{\pi}(Y)})\\&=\frac{1}{M}\left[\int \left(f(y)\frac{\pi(y)}{\widetilde{\pi}(y)}\right)^2\widetilde{\pi}(y)dy-\pi[f]^2\right]
\end{aligned}
\end{equation*}
$$

We notice that

$$
\int \left(f(y)\frac{\pi(y)}{\widetilde{\pi}(y)}\right)^2\widetilde{\pi}(y)dy\geq \left(\int \vert f(y)\vert \pi(y)\right)^2=\pi[\vert f\vert]^2
$$

The first inequality is by Jensen's Inequality. The second quality holds if

$$
\widetilde{\pi}(x)=\frac{\vert f(x)\vert \pi(x)}{\int\vert f(y)\vert \pi(y)dy}
$$

So $\widetilde{\pi} \propto \vert f\vert \pi$ is the optimal choice. However, we can not sample independently from $\widetilde{\pi}$ and we even do not know the ratio $\frac{\pi}{\widetilde{\pi}}$. Our method is to approximate the $\widetilde{\pi}$ as in the following section.

Although we do not know the ratio, sometimes we know this up to a multiplicative constant.

If $\pi\propto p$ and $\widetilde{\pi}\propto\widetilde{p}$, we can use the  __Autonormalized IS__:

$$
\frac{\widetilde{f_M}}{\widetilde{1_M}}=\frac{\frac{1}{M}\sum_{j=1}^Mf(Y^{(j)})\frac{\pi(Y^{(j)})}{\widetilde{\pi}(Y^{(j)})}}{\frac{1}{M}\sum_{j=1}^M\frac{\pi(Y^{(j)})}{\widetilde{\pi}(Y^{(j)})}}=\frac{\frac{1}{M}\sum_{j=1}^Mf(Y^{(j)})\frac{p(Y^{(j)})}{\widetilde{p}(Y^{(j)})}}{\frac{1}{M}\sum_{j=1}^M\frac{p(Y^{(j)})}{\widetilde{p}(Y^{(j)})}}
$$

## An Asymptotic Rare Event Setting

In this section, we use an __asymptotic setting__ to quantify the performance of choosing $\widetilde{\pi}$

Let $V$ and $g$ be upper semi-continuous functions. We also assume that g is bounded below and $\text{lim inf}_{\|x\|\rightarrow \infty}\frac{V(x)}{\|x\|}=\infty$ and $\text{inf}_x V(x)=0$. Let $\pi_{\epsilon}\propto e^{-V(x)/\epsilon}$ and $f_{\epsilon}(x)=e^{-g(x)/\epsilon}$. However, it is hard to estimate $\pi_{\epsilon}[f_{\epsilon}]$ for small $\epsilon$ as we will show below.

Before that we will introduce an important theorem without proof.

> ### **Laplace Principle**
>
> For any upper semi-continuous fucntion $h$ with $\text{lim}_{\|x\|\rightarrow \infty}\frac{h(x)}{\|x\|}=\infty$ and $h_{}=\text{inf}_x h(x)>\infty$, then we have
>
>$$
>\text{lim}_{\epsilon\rightarrow 0} \;\epsilon \,\text{log}\int e^{-h(x)/\epsilon} dx = -h_{*}
>$$

This implies 

$$
\int e^{-h(x)/\epsilon} dx = e^{-(h_{*}+\omicron(1))/\epsilon}
$$

We call the term $e^{-\omicron(1)/\epsilon}$ the prefactor. In fact, we can characterize the prefactor more precisely. For example,

> ### **Saddle Point Approximation**
>
>Based on the conditions of the Laplace Principle, we further assume that $h\in C^2$ and has a unique global minimum at $x_h$. We also assume that $D^2h(x_h)$ is positive definite, then we have
>
>$$
>\int e^{-h(x)/\epsilon} dx = \epsilon^{d/2}e^{-h(x_h)/\epsilon}\left(\frac{(2\pi)^{d/2}}{\vert Dh(x_h)\vert^{1/2}}+\omicron(1)\right)
>$$

Finally, we use the Laplace Principle to find the decay rate of $\pi_{\epsilon}[f_{\epsilon}]$. Note that $h:=V+g$ is upper semi-continuous and bounded below, so we have

$$
\epsilon\text{log}\int e^{-V(x)/\epsilon}e^{-g(x)/\epsilon}\rightarrow -\text{inf}_x \{V(x)+g(x)\}
$$

so we get 

$$
\epsilon\text{log}(\pi_{\epsilon}[f_{\epsilon}])\rightarrow -\text{inf}_x \{V(x)+g(x)\}
$$

In a similar style we consider $\pi_{\epsilon}[f_{\epsilon}^2]$ by applying Laplace Principle to $h=V+2g$ and we get 

$$
\epsilon\text{log}(\pi_{\epsilon}[f_{\epsilon}^2])\rightarrow -\text{inf}_x \{V(x)+2g(x)\}
$$

Since the relative variance of $\overline{f_M}$ is 

$$
\frac{\text{var}(\overline{f_M})}{\pi_{\epsilon}[f_{\epsilon}]^2}=\frac{1}{M}\left(\frac{\pi_{\epsilon}[f_{\epsilon}^2]}{\pi_{\epsilon}[f_{\epsilon}]^2}-1\right)
$$

we have

$$
\epsilon\text{log}\frac{\text{var}(\overline{f_M})}{\pi_{\epsilon}[f_{\epsilon}]^2}\rightarrow \text{inf}_x \{2V(x)+2g(x)\}-\text{inf}_x \{V(x)+2g(x)\}
$$

we denote the right hand side as $\gamma$. If $\gamma>0$, the relative variance will grow exponentially with rate $\gamma$, which is bad. We will explore some methods to eliminate the growth.