---
layout: distill
title: "I. Introduction to Rare Event Simulation"
description: "TODO"
tags: Rare-Event-Simulation
categories: lecture-notes
giscus_comments: true
date: 2024-07-15 09:00:00-0400
featured: false

authors:
  - name: Yineng Chen
    url: "https://chernyn.github.io/"
    affiliations:
      name: Purdue University
      url: https://www.purdue.edu

bibliography: 2024-07-10-mist.bib

toc:
  - name: Motivation
  - name: The Static Settings
  - name: Concentration Bound
  - name: Why Rare Event Statistics Difficult to Estimate

---

These series of notes were taken by the author in the course "__Computational methods for sampling and analyzing rare and extreme events__" taught by __Prof. Jonathan Weare__ during the 2024 PKU Summer School on Applied Mathematics.

## Motivation

In this blog, we explore to estimate the statistics of rare events. The difficulty lies in the fact that the problem requires the resolution of short time scale phenomenon compared to the whole time scale of the event. The main idea is to learn how to use a computer model to generate samples from the low probability tail of a given distribution.

## The Static Settings

Given a density $\pi$, We need to estimate
$$
\pi[f]:=\mathbb{E}_{\pi}[f(X)]=\int f(x)\pi(x)dx
$$
The concentrated problem is the case that $f$ is large only in regions where $\pi$ is small, so that $\pi[f]$ is also small.

The most basic idea is to use the __Monte-Carlo method__ to estimate $\pi[f]$:
$$
\overline{f_M}=\frac{1}{M}\sum_{j=1}^Mf(X^{(j)})
$$
where $X^{(j)}$ are i.i.d samples from $\pi$. In this way, we get an __unbiased estimator__:
$$
\mathbb{E}[\overline{f_M}]=\frac{1}{M}\sum_{j=1}^M\mathbb{E}_{\pi}f(X^{(j)})=\pi[f]
$$
Next, we consider the "convergence" of $\overline{f_M}$ to $\pi[f]$. For this purpose, we will introduce some notations. Let $Y$ be any estimator of $\pi[f]$, we get the following equation by easy computation:
$$
\mathbb{E}[|Y-\pi[f]|^2]=\mathbb{E}[|Y-\mathbb{E}[Y]|^2]+|\pi[f]-\mathbb{E}[Y]|^2
$$
The term in the left hand side of this equation is called __mean squared error (MSE)__. The first term of the right hand side is the variance of $Y$ ($\text{var}(Y)$), and the second term is the square of bias. Then, for $\overline{f_M}$: 
$$
\text{MSE}(\overline{f_M})=\text{var}_{\pi}(\overline{f_M})=\frac{1}{M^2}\sum_{i=1}^M\text{var}_{\pi}(f(X^{(i)}))=\frac{1}{M}\text{var}_{\pi}(f(X))
$$
Thus we know that $\text{MSE}(\overline{f_M})\rightarrow 0$ as $M\rightarrow \infty$, as long as $\text{var}_{\pi}(f(X))< \infty$. Then, using Chebyshev inequality we get


## Concentration Bound

First, we need to introduce the moment generating fucntion:
$$
\Lambda(t)=\text{log}\,\mathbb{E}^{tf(X)}<\infty,\,\text{for}\, |t|<\delta
$$
Then, by Markov's inequality, $\forall t>0$ we have
$$
\begin{equation*}
\begin{aligned}
\text{P}(|\overline{f_M}-\pi[f]|>\epsilon)&=\text{P}(e^{Mt(\overline{f_M}-\pi[f])}>e^{Mt\epsilon})\\&\leq e^{-Mt\epsilon}\mathbb{E}[e^{Mt(\overline{f_M}-\pi[f])}]\\&=e^{-Mt\epsilon}\mathbb{E}[e^{t(\overline{f_M}-\pi[f])}]^M\\&=e^{-M(t(\epsilon+\pi[f])-\Lambda(t))}\\&\leq e^{-M\gamma(\epsilon)}
\end{aligned}
\end{equation*}
$$
where $\gamma(\epsilon)$ is defined to be $\text{sup}_{t>0}\{t(\epsilon+\pi[f])-\Lambda(t)\}$. Therefore, $\text{P}(|\overline{f_M}-\pi[f]|\rightarrow 0$ exponentially fast as long as $\gamma(\epsilon)>0$.

## Why Rare Event Statistics Difficult to Estimate

Since $\pi[f]$ is small we want to control the relative accuracy:
$$
\mathbb{P}\left(\left|\frac{\overline{f_M}-\pi[f]}{\pi[f]}\right|>\epsilon\right) \text{or} \;\mathbb{E}\left[\left|\frac{\overline{f_M}-\pi[f]}{\pi[f]}\right|^2\right]
$$
Sometimes we can alos consider
$
\text{log}\,\frac{\overline{f_M}}{\pi[f]} 
$
instead of $\left|\frac{\overline{f_M}-\pi[f]}{\pi[f]}\right|$.

Notice that
$$
\frac{\text{var}(\overline{f_M})}{\pi[f]^2}=\frac{1}{M}\left(\frac{\pi[f^2]}{\pi[f]^2}-1\right)
$$

Consider the case that $f(\cdot)$ is an indicator function, that is
$$
\begin{equation*}
f(x)
=\left\{
\begin{array}{ll}
1 & \text{if}\;{x} \in S \\
0 & \text{if}\;{x} \notin S
\end{array}\right.
\end{equation*}
$$
In this way, $\pi[f^2]=\pi[f]^2$ and we get
$$
\frac{\text{var}(\overline{f_M})}{\pi[f]^2}=\frac{1}{M}\left(\frac{1}{\pi[f]}-1\right)
$$
When $\pi[f]<<1$ (as in the case of rare events), the right hand side of the above equation is bad.
