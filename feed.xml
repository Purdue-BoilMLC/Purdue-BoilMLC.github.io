<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://purdue-boilmlc.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://purdue-boilmlc.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-10-06T04:19:19+00:00</updated><id>https://purdue-boilmlc.github.io/feed.xml</id><title type="html">Boilermakers Machine Learning Collaborative (Boil-MLC)</title><subtitle>The Boilermakers Machine Learning Collaborative at Purdue University is a student-led interdisciplinary initiative established to bridge theoretical foundations with practical applications in the field of machine learning. Targeted primarily at PhD and Masters students, BoilMLC aims to leverage the diverse academic backgrounds of its members to foster innovative research and enhance the academic and professional stature of its participants. </subtitle><entry><title type="html">Safeguarding Machine Learning Models: An Introduction to MIST</title><link href="https://purdue-boilmlc.github.io/blog/2024/MIST/" rel="alternate" type="text/html" title="Safeguarding Machine Learning Models: An Introduction to MIST"/><published>2024-07-10T13:00:00+00:00</published><updated>2024-07-10T13:00:00+00:00</updated><id>https://purdue-boilmlc.github.io/blog/2024/MIST</id><content type="html" xml:base="https://purdue-boilmlc.github.io/blog/2024/MIST/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>In the current landscape of rapidly evolving AI application fields, machine learning models have become integral to numerous applications, from voice assistants<d-cite key="lopez2018alexa"></d-cite> and image generation to advanced conversational AI<d-cite key="rombach2022"></d-cite><d-cite key="kang2023scaling"></d-cite>. However, the widespread adoption of these models has raised significant privacy concerns. One of the most pressing issues is the vulnerability to Membership Inference (MI) attacks<d-cite key="shokri2017"></d-cite>, where an adversary attempts to determine whether a specific data instance was used to train a machine learning model. This can lead to serious privacy breaches, particularly when dealing with sensitive personal information.</p> <p>The challenge for researchers and practitioners lies in developing robust defenses against these attacks <em>without significantly compromising model performance</em>. Traditional defense mechanisms often result in reduced accuracy as they aim to prevent the model from overfitting to the training data. In response to this challenge, a novel approach called <strong>Membership-Invariant Subspace Training (MIST)</strong><d-cite key="li2024mistdefendingmembershipinference"></d-cite> has been proposed. MIST is designed to protect vulnerable training instances while maintaining high accuracy.</p> <p>This blog post aims to provide an overview of MI attacks and the MIST defense mechanism. We will begin by examining the fundamental concepts and mathematical principles underlying MI attacks. Then, we will explore how MIST offers an effective defense against these attacks, striking a balance between the critical needs of privacy and performance in machine learning models.</p> <p>Our discussion will build upon existing research in the field, including seminal works on privacy-preserving machine learning and recent advancements in defense strategies against inference attacks. By the end of this post, readers will have a deeper understanding of the privacy challenges facing machine learning models and a clear insight into how MIST addresses these concerns.</p> <h3 id="background-and-previous-work">Background and Previous Work</h3> <p>Membership Inference (MI) attacks were first systematically studied by Shokri et al.<d-cite key="shokri2017"></d-cite>, who demonstrated that adversaries could exploit the differences in a model’s confidence scores between training and non-training data to infer membership status . Following this seminal work, various defenses were proposed, each attempting to obscure the distinctions between training and non-training data.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/2024-07-10-mist/shokri-get_shadows-480.webp 480w,/assets/img/blog/2024-07-10-mist/shokri-get_shadows-800.webp 800w,/assets/img/blog/2024-07-10-mist/shokri-get_shadows-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/2024-07-10-mist/shokri-get_shadows.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/2024-07-10-mist/shokri-overview-480.webp 480w,/assets/img/blog/2024-07-10-mist/shokri-overview-800.webp 800w,/assets/img/blog/2024-07-10-mist/shokri-overview-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/2024-07-10-mist/shokri-overview.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Left: In Shokri et al.'s work<d-cite key="shokri2017"></d-cite>, the attacker can use exactly the same service (e.g., Google Prediction API) to train the shadow model as was used to train the target model. Right: Membership inference attack in the black-box setting<d-cite key="shokri2017"></d-cite>. </div> <p>One common defense strategy is differential privacy, which adds noise to the training process to mask the contributions of individual training examples. Abadi et al.<d-cite key="Abadi_2016"></d-cite> applied this technique to deep learning, providing theoretical guarantees against MI attacks but often at the cost of reduced model utility. Recent research has explored the use of adversarial training, where models are trained with adversarial examples that aim to minimize the information leakage from the model’s outputs​. While this can enhance robustness against MI attacks, it introduces additional computational overhead and complexity.</p> <p>Despite these advancements, a fundamental challenge remains:</p> <blockquote><p>How can we efficiently learn membership-invariant classifiers to defend against MI for all vulnerable instances?</p><cite><a class="citation" href="#li2024mistdefendingmembershipinference">(Li et al., 2024)</a></cite></blockquote> <p>This is where Membership-Invariant Subspace Training (MIST) comes into play. MIST leverages recent advancements in representation learning to focus on the instances most susceptible to MI attacks, offering a refined balance between privacy and accuracy.</p> <p>In this blog, we will explore the mathematical underpinnings and intuitive concepts behind MI attacks and delve into how MIST effectively addresses these challenges.</p> <h2 id="understanding-membership-inference-attacks">Understanding Membership Inference Attacks</h2> <p>To appreciate the innovation behind MIST, it is crucial to understand the foundation of Membership Inference (MI) attacks.</p> <h3 id="formulation-of-mi-attacks">Formulation of MI Attacks</h3> <p>In a Membership Inference Attack, an adversary aims to determine if a specific instance $(x, y)$ was part of the training dataset $D$ used to train a machine learning model $F$. Formally, the adversary seeks to infer the membership status of an instance by analyzing the model’s behavior.</p> <p>Consider a machine learning model $F$ trained on a dataset $D = {(x_i, y_i)}_{i=1}^{n}$. The model $F$ is optimized to minimize a loss function $L(F(x_i), y_i)$, which measures the difference between the model’s prediction and the true label. The goal of the training process is to reduce this loss for all instances in the training set.</p> <p>However, this optimization leads to a critical vulnerability: instances in the training set typically exhibit lower loss compared to instances outside the training set. An adversary can exploit this difference by querying the model with an instance $x$ and observing the resulting loss $L(F(x), y)$. If the loss is significantly lower, the adversary may infer that $x$ was likely part of the training set.</p> <p>Mathematically, an MI attack can be represented as:</p> \[\text{Infer}(\text{Membership}) = \begin{cases} \text{Member} &amp; \text{if } L(F(x), y) &lt; \tau \\ \text{Non-member} &amp; \text{if } L(F(x), y) \geq \tau \end{cases}\] <p>where $\tau$ is a threshold chosen based on the model’s loss distribution.</p> <p>Intuitively, MI attacks take advantage of the fact that machine learning models are designed to perform well on their training data. When a model is trained, it “learns” the patterns and characteristics of the training instances, resulting in high confidence and low loss for these instances. Conversely, instances not seen during training generally result in higher loss as the model is less familiar with them.</p> <p>Imagine a student who has studied specific questions for an exam. If the exact same questions appear on the test, the student will perform exceptionally well, similar to how a model exhibits low loss on training data. However, if new questions appear, the student’s performance may decline, akin to the model showing higher loss on non-training instances. An MI attack is analogous to an observer trying to determine if a particular question was part of the student’s study material based on the student’s performance.</p> <h3 id="existing-defenses-and-their-limitations">Existing Defenses and Their Limitations</h3> <p>Existing defenses against Membership Inference (MI) attacks typically fall into two categories: <strong>regularization techniques</strong> and <strong>adversarial training</strong>. Each has its strengths and limitations.</p> <p><strong>1. Regularization Techniques</strong></p> <p>Regularization techniques aim to prevent the model from overfitting the training data, thus making it harder for an adversary to distinguish between training and non-training instances.</p> <ul> <li><strong>Dropout:</strong> Dropout is a regularization method where a fraction of neurons is randomly dropped during training. This prevents the network from becoming too reliant on specific neurons, promoting a more generalized learning.</li> </ul> <blockquote class="block-warning"> <h5 id="limitation"><strong>Limitation</strong></h5> <p>While dropout can reduce overfitting, it also can degrade the model’s performance if not tuned properly. Additionally, dropout alone might not sufficiently obscure the distinctiveness of certain vulnerable instances.</p> </blockquote> <ul> <li><strong>Label Smoothing:</strong> This technique involves modifying the training labels to be less confident. Instead of using a one-hot encoded vector, labels are smoothed to distribute some probability mass to all classes.</li> </ul> <blockquote class="block-warning"> <h5 id="limitation-1"><strong>Limitation</strong></h5> <p>Label smoothing can reduce the confidence gap between training and non-training instances, but it may also harm the model’s accuracy by making the learning process less precise.</p> </blockquote> <ul> <li><strong>Differential Privacy (DP):</strong> DP adds noise to the model’s gradients or outputs to obscure the contribution of individual instances. It provides strong theoretical guarantees about the privacy of training data.</li> </ul> <blockquote class="block-warning"> <h5 id="limitation-2"><strong>Limitation</strong></h5> <p>The main drawback of DP is the trade-off between privacy and accuracy. High levels of noise ensure better privacy but significantly degrade model performance. Additionally, the implementation complexity of DP can be a barrier.</p> </blockquote> <p><strong>2. Adversarial Training</strong></p> <p>Adversarial training involves creating a separate adversary model that attempts to perform MI attacks during the training phase. The primary model is then trained to be robust against this adversary.</p> <ul> <li> <p><strong>Adversarial Regularization:</strong> In this method, an adversary is trained alongside the primary model to identify whether instances are from the training set. The primary model is then penalized if the adversary successfully distinguishes training instances.</p> <p><strong>Limitation:</strong> Adversarial regularization can be effective but often leads to increased training complexity and time. Furthermore, it may not fully protect against more sophisticated MI attacks that the adversarial model wasn’t trained to simulate.</p> </li> </ul> <p><strong>Limitations Across the Board</strong></p> <p>While these defenses offer some protection against MI attacks, they often come with significant trade-offs:</p> <ul> <li> <p><strong>Accuracy Reduction:</strong> Many defenses reduce model accuracy because they prevent the model from fitting the training data too closely. This is especially problematic in applications where high accuracy is critical.</p> </li> <li> <p><strong>Computational Overhead:</strong> Techniques like adversarial training and differential privacy introduce additional computational overhead, making the training process more resource-intensive and time-consuming.</p> </li> <li> <p><strong>Inadequate Protection for Distinctive Instances:</strong> Most existing methods do not specifically address the differential vulnerability of training instances. They apply a uniform approach across all instances, which can leave the most distinctive and vulnerable instances insufficiently protected.</p> </li> </ul> <p>The MIST approach is designed to overcome these limitations by focusing on the most vulnerable instances and using a subspace learning framework to provide robust protection without sacrificing model accuracy.</p> <h2 id="in-depth-look-at-mist">In-depth Look at MIST</h2> <h3 id="conceptual-overview">Conceptual Overview</h3> <p>The crux of MIST lies in addressing the differential vulnerability of training instances to MI attacks. Recognizing that not all training instances are equally susceptible, MIST focuses on protecting those with higher vulnerability, termed “distinctive” instances. This is achieved through a multi-phase process that blends subspace learning with counterfactually-invariant representations.</p> <h3 id="subspace-learning-and-counterfactual-invariance">Subspace Learning and Counterfactual Invariance</h3> <ol> <li><strong>Subspace Creation:</strong> <ul> <li>The training dataset $D$ is divided into multiple subsets $D_1, D_2, \ldots, D_k$.</li> <li>Independent submodels $F_1, F_2, \ldots, F_k$ are trained on these subsets. Each submodel $F_i$ learns from its respective subset $D_i$, creating a diverse subspace of models.</li> </ul> <p>Mathematically, for each submodel $F_i$ trained on $D_i$: \(F_i = \text{Train}(D_i)\)</p> </li> <li><strong>Gradient Adjustment:</strong> <ul> <li>To ensure that distinctive instances do not disproportionately influence the final model, gradient adjustments are performed.</li> <li>For each submodel $F_i$, the gradients are adjusted to align its predictions with the average predictions of other submodels.</li> </ul> <p>Let $G_i$ denote the gradient of the loss function for submodel $F_i$: \(G_i = \nabla L(F_i(x), y)\) The adjusted gradient $\hat{G}_i$ is computed as: \(\hat{G}_i = G_i - \eta \sum_{j \neq i} (F_i(x) - F_j(x))\) where $\eta$ is a learning rate parameter.</p> </li> <li><strong>Averaging Submodels:</strong> <ul> <li>The submodels are averaged to form the final model $F$.</li> <li>This averaging process ensures that the output on distinctive instances approximates the scenario where these instances are not included in the training, thereby reducing their vulnerability.</li> </ul> <p>The final model $F$ is given by:</p> \[F = \frac{1}{k} \sum_{i=1}^{k} F_i\] </li> </ol> <h3 id="mathematical-justification">Mathematical Justification</h3> <p>The gradient adjustment step is pivotal in achieving counterfactually-invariant representations. By aligning the predictions of submodels, the influence of distinctive instances is diluted across the subspace. This reduces the variance in loss for these instances, making it harder for an adversary to distinguish members from non-members based on loss values.</p> <p>The averaging of submodels further reinforces this invariance by ensuring that the final model’s behavior is a composite of multiple independent models. This composite nature helps in masking the presence of distinctive instances, thereby enhancing privacy protection.</p> <p>THerefore, MIST represents a substantial advancement in defending against MI attacks, offering robust protection for vulnerable instances without the typical accuracy trade-offs. Future research could explore further optimizations and the application of MIST in diverse machine learning scenarios.</p>]]></content><author><name>Yuetian Chen</name></author><category term="take-aways"/><category term="Security"/><summary type="html"><![CDATA[A paper review on MIST: Defending Against Membership Inference Attacks Through Membership-Invariant Subspace Training]]></summary></entry><entry><title type="html">A brief introduction to Deep Q-Learning</title><link href="https://purdue-boilmlc.github.io/blog/2024/DQN/" rel="alternate" type="text/html" title="A brief introduction to Deep Q-Learning"/><published>2024-07-05T13:00:00+00:00</published><updated>2024-07-05T13:00:00+00:00</updated><id>https://purdue-boilmlc.github.io/blog/2024/DQN</id><content type="html" xml:base="https://purdue-boilmlc.github.io/blog/2024/DQN/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>This paper proposes a basic end-to-end high-dimensional pixel control strategy based on the reinforcement learning framework of the Q-Learning algorithm in the 1992 paper, combined with the powerful image processing capabilities of convolutional neural networks.</p> <p>The above framework can handle high-dimensional data such as pixels, but the correlation between the data and the instability of data distribution in reinforcement learning itself have not been solved. The author uses the experience replay mechanism to solve this problem based on the 1993 paper 2. This experience replay mechanism hopes that the distribution of reinforcement learning can slowly transition from the random data at the beginning to the current situation with better results.</p> <h2 id="q-learning">Q-Learning</h2> <h3 id="definition">Definition</h3> <ul> <li>Q-function maps state and action pairs to expected reward.</li> <li>For some state $s$ and action $a$, Q-function $Q(s,a)$ represents the expected accumulated reward when the agent choose action $a$ under the state $s$, and follow some policy until the end of the task.</li> <li>If we have an optimal function $Q^{*}(s,a)$, then we can get the largest reward if the agent take action $a$ of the $Q^{*}(s,a)$ under the state of $s$. This is the learning target of Q-Learning.</li> </ul> <h3 id="algorithm-procedure">Algorithm Procedure</h3> <ol> <li>Initialization of the Q function <ul> <li>Represent it by a Q-table or Q-network.</li> <li>Generate the the initial value.</li> </ul> </li> <li>Interation between the agent and the environment. <ul> <li>The agent observes the current state $s_t$.</li> <li>The agent selects an action $a_t$ according to the given policy, like the $\epsilon$-greedy policy.</li> <li>Take action $a_t$, and the environment returns the reward $r_t$ and the next state $s_{t+1}$.</li> <li>Experience $(s_t, a_t, r_t, s_{t+1})$ is restored in the replay pool.</li> </ul> </li> <li>Training <ul> <li>Sample a batch of (s, a, r, s’) in the experience replay pool.</li> <li>Compute target for each experience: \(y = r + \gamma * max_{a'} Q(s', a')\) where \gamma is the discount factor, indicating the importance attached to future rewards</li> <li>compute TD error: \(e = y - Q(s, a)\)</li> <li>Update Q-function based on TD error: \(Q(s, a) &lt;- Q(s, a) + α * e\) where $\alpha$ is the learning rate</li> </ul> </li> <li>Repeat the procedure 2 and 3, until the Q-function converges or when it reaches the given training step.</li> </ol>]]></content><author><name>Yineng Chen</name></author><category term="take-aways"/><category term="Reinforcement-Learning"/><summary type="html"><![CDATA[A paper review on DQN]]></summary></entry><entry><title type="html">Understanding Wasserstein GANs: A Mathematical Perspective</title><link href="https://purdue-boilmlc.github.io/blog/2024/WGAN/" rel="alternate" type="text/html" title="Understanding Wasserstein GANs: A Mathematical Perspective"/><published>2024-06-24T13:00:00+00:00</published><updated>2024-06-24T13:00:00+00:00</updated><id>https://purdue-boilmlc.github.io/blog/2024/WGAN</id><content type="html" xml:base="https://purdue-boilmlc.github.io/blog/2024/WGAN/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Generative Adversarial Networks (GANs)<d-cite key="goodfellow2014generative"></d-cite> have revolutionized the field of generative modeling by enabling the creation of highly realistic images, videos, and other media. Despite their success, standard GANs frequently face significant training challenges, such as instability, mode collapse, and difficulties in achieving convergence. These issues often stem from the choice of divergence metrics used to measure the difference between real and generated data distributions.</p> <p>In this blog post, we explore how Wasserstein GANs (WGANs)<d-cite key="arjovsky2017wasserstein"></d-cite> address these challenges by leveraging the Wasserstein distance, leading to more stable training and superior generative performance. We delve into the mathematical foundations of WGANs, discuss their improvements over standard GANs, and examine their practical implications.</p> <h2 id="overcoming-challenges-in-standard-gans">Overcoming Challenges in Standard GANs</h2> <p>Standard GANs consist of two neural networks—the generator (G) and the discriminator (D)—engaged in a minimax game. The generator aims to create samples indistinguishable from real data, while the discriminator strives to differentiate between real and generated samples. The loss functions for the generator and discriminator can be expressed as:</p> \[\label{eq:gan_loss} \begin{aligned} \min_G \max_D &amp; \, \mathbb{E}_{x \sim P_r} [\log D(x)] + \mathbb{E}_{\hat{x} \sim P_g} [\log (1 - D(\hat{x}) )], \end{aligned}\] <p>where \(P_r\) denotes the real data distribution, and \(P_g\) is the distribution induced by the generator over the data space.</p> <h3 id="common-issues-in-standard-gan-training">Common Issues in Standard GAN Training</h3> <ol> <li><strong>Mode Collapse</strong>: The generator produces a limited diversity of samples, causing it to capture only a few modes of the real data distribution.</li> <li><strong>Training Instability</strong>: The adversarial nature of the GAN training often leads to oscillations and lack of convergence.</li> <li><strong>Vanishing Gradients</strong>: When the discriminator becomes too effective, the generator receives negligible gradient updates, hindering its improvement.</li> </ol> <p>These problems largely arise from the nature of the Jensen-Shannon (JS) divergence used in standard GANs. When the supports of \(P_r\) and \(P_g\) do not overlap, the JS divergence becomes constant, leading to vanishing gradients.</p> <h2 id="the-wasserstein-distance">The Wasserstein Distance</h2> <p>Wasserstein GANs propose an elegant solution by replacing the JS divergence with the Wasserstein distance (also known as the Earth Mover’s distance), which measures the cost of transforming one distribution into another. This metric remains meaningful even when the distributions have non-overlapping supports.</p> <h3 id="mathematical-formulation">Mathematical Formulation</h3> <p>The Wasserstein distance between two probability distributions \(P_r\) and \(P_g\) is defined as:</p> \[\label{eq:wasserstein} W(P_r, P_g) = \inf_{\gamma \in \Pi(P_r, P_g)} \mathbb{E}_{(x, y) \sim \gamma}[\|x - y\|],\] <p>where \(\Pi(P_r, P_g)\) is the set of all joint distributions \(\gamma(x, y)\) whose marginals are \(P_r\) and \(P_g\), respectively. Intuitively, it represents the minimum amount of “work” needed to transform \(P_g\) into \(P_r\), considering the cost of moving mass from point \(x\) to \(y\).</p> <h3 id="kantorovich-rubinstein-duality">Kantorovich-Rubinstein Duality</h3> <p>In practice, directly computing the Wasserstein distance is intractable. Instead, the problem can be reformulated using Kantorovich-Rubinstein duality<d-cite key="villani2009optimal"></d-cite>:</p> \[\label{eq:kantorovich} W(P_r, P_g) = \sup_{\|f\|_L \leq 1} \mathbb{E}_{x \sim P_r}[f(x)] - \mathbb{E}_{x \sim P_g}[f(x)],\] <p>where the supremum is over all 1-Lipschitz functions \(f\).</p> <p>In the context of WGANs, the discriminator \(D\) is used to approximate the optimal 1-Lipschitz function \(f\). Thus, the loss functions for the generator and discriminator can be written as:</p> \[\label{eq:wgan_loss} \begin{aligned} \max_D &amp; \, \mathbb{E}_{x \sim P_r}[D(x)] - \mathbb{E}_{\hat{x} \sim P_g}[D(\hat{x})], \\ \min_G &amp; \, \mathbb{E}_{\hat{x} \sim P_g}[D(\hat{x})]. \end{aligned}\] <h3 id="enforcing-the-1-lipschitz-constraint">Enforcing the 1-Lipschitz Constraint</h3> <p>A critical requirement for the discriminator in WGANs is the 1-Lipschitz constraint. In the original WGAN formulation, this constraint was enforced by clipping the weights of the discriminator to a narrow range. However, this clipping can lead to several practical problems, such as underfitting.</p> <p>An improvement to this approach is the gradient penalty method<d-cite key="gulrajani2017improved"></d-cite>. Here, the 1-Lipschitz constraint is enforced by adding a penalty to the loss function, ensuring that the gradient of the discriminator with respect to its input is close to 1:</p> \[\label{eq:gradient_penalty} \mathcal{L}_{\text{GP}} = \lambda \mathbb{E}_{\hat{x} \sim P_{\hat{x}}}\left[(\|\nabla_{\hat{x}} D(\hat{x})\|_2 - 1)^2\right],\] <p>where \(P_{\hat{x}}\) is the distribution of points along straight lines between pairs of points sampled from \(P_r\) and \(P_g\).</p> <h2 id="conclusion">Conclusion</h2> <p>Wasserstein GANs mark a significant advancement in the field of generative modeling, especially for image synthesis. By addressing the fundamental shortcomings of standard GANs with the imposition of the Wasserstein distance, WGANs achieve more stable training dynamics and generate higher quality samples.</p> <p>The theoretical robustness rooted in optimal transport theory and practical improvements like gradient penalty make WGANs a critical tool in the arsenal of machine learning practitioners. As research in this area progresses, we can expect further refinements and hybrid models that continue to push the boundaries of what generative models can achieve.</p> <p>In subsequent posts, we will explore other transformative advancements in AI and machine learning, providing a comprehensive view of the state-of-the-art technologies shaping our world.</p>]]></content><author><name>Yuetian Chen</name></author><category term="take-aways"/><category term="Computational-Creactivity"/><category term="Generative-AI"/><category term="GANs"/><summary type="html"><![CDATA[A paper review on Wasserstein GAN]]></summary></entry><entry><title type="html">Gopher: Scaling Language Models to New Heights</title><link href="https://purdue-boilmlc.github.io/blog/2024/Gopher/" rel="alternate" type="text/html" title="Gopher: Scaling Language Models to New Heights"/><published>2024-05-29T13:00:00+00:00</published><updated>2024-05-29T13:00:00+00:00</updated><id>https://purdue-boilmlc.github.io/blog/2024/Gopher</id><content type="html" xml:base="https://purdue-boilmlc.github.io/blog/2024/Gopher/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Recent years have seen tremendous progress in the fields of natural language processing (NLP) and generative modeling. Language models like GPT-3<d-cite key="brown2020language"></d-cite> and PaLM<d-cite key="chowdhery2022palm"></d-cite> have demonstrated remarkable capabilities in understanding and generating human-like text. In this blog post, we delve into the groundbreaking work on the Gopher model<d-cite key="rae2022scaling"></d-cite> developed by DeepMind. We will explore the architecture, key insights from this work, and its implications for the future of language models.</p> <h2 id="scaling-language-models">Scaling Language Models</h2> <p>Language models form the backbone of modern NLP systems. By learning from vast corpora of text data, these models acquire a broad understanding of language that can be applied to diverse downstream tasks. The Gopher model<d-cite key="rae2022scaling"></d-cite>, developed by DeepMind, represents an important milestone in the scaling of language models.</p> <h3 id="the-gopher-architecture">The Gopher Architecture</h3> <p>Gopher is a large-scale transformer-based language model. It follows the decoder-only architecture that has become standard for models like GPT-3. However, the Gopher architecture incorporates several notable and innovative modifications which distinguish it from its predecessors:</p> <ol> <li> <p><strong>Scaling in Size</strong>: Gopher ranges up to 280 billion parameters, significantly larger than many previous models. This increase in scale aims to leverage more data and model capacity to improve performance across diverse tasks.</p> </li> <li> <p><strong>RMSNorm instead of LayerNorm</strong>: Gopher employs RMSNorm<d-cite key="zhang2019root"></d-cite> instead of the more commonly used LayerNorm. RMSNorm (Root Mean Square Normalization) is computationally simpler and may lead to more stable gradients, enhancing the performance of deep networks. Mathematically, RMSNorm normalizes the inputs based on their root mean square, promoting better normalization properties by focusing on the scale of the neurons’ activations.</p> \[\text{RMSNorm}(\mathbf{x}) = \frac{\mathbf{x}}{\text{RMS}(\mathbf{x})}\] <p>where \(\text{RMS}(\mathbf{x}) = \sqrt{\frac{1}{d} \sum_{i=1}^{d} x_i^2}\), and \(d\) is the dimension of the input vector \(\mathbf{x}\).</p> </li> <li> <p><strong>Relative Positional Encodings</strong>: Gopher utilizes relative positional encodings instead of absolute positional encodings. This method allows the model to better capture the relative distances and dependencies between tokens within the input sequence. Relative positional encodings maintain consistent performance regardless of the sequence length.</p> </li> </ol> <h3 id="key-insights-from-gopher">Key Insights from Gopher</h3> <p>The Gopher paper offers several valuable insights into the behavior and benefits of scaling up language models:</p> <ol> <li> <p><strong>Task Performance</strong>: The benefits of scale are most pronounced on tasks such as reading comprehension, fact-checking, and toxicity detection. The improvements in logical and mathematical reasoning are more modest. This discrepancy can be attributed to the nature and distribution of training data, highlighting the importance of dataset curation for developing balanced and effective language models.</p> </li> <li> <p><strong>Generalization</strong>: Larger models like Gopher generalize better across a diverse set of tasks. They demonstrate enhanced zero-shot and few-shot learning capabilities, where the model has to perform tasks with minimal or no task-specific training data.</p> </li> <li> <p><strong>Ethical Considerations</strong>: As models scale, ethical and societal implications become more significant. The Gopher paper emphasizes the need for comprehensive evaluations of model biases and the importance of developing frameworks for responsible AI.</p> </li> </ol> <p>The Gopher model underscores the immense potential of scaling up language models in terms of parameter size and data volume. It points to a future where such models can serve as flexible, general-purpose tools for a wide range of applications.</p> <h2 id="conclusion">Conclusion</h2> <p>Gopher represents a significant advancement in the field of language modeling, pushing the boundaries of what large-scale models can achieve. Its novel architectural choices and insightful analysis provide a valuable blueprint for future research and development in NLP. As we continue to scale up models and refine our training techniques, we will unlock new frontiers in machine intelligence, transforming the capabilities of AI systems across various domains. However, it is crucial to address the ethical and societal implications of these powerful technologies, ensuring their development and deployment benefit all of humanity.</p>]]></content><author><name>Yuetian Chen</name></author><category term="take-aways"/><category term="NLP"/><category term="Transformer"/><category term="LLM"/><summary type="html"><![CDATA[A paper review on Scaling Language Models: Methods, Analysis & Insights from Training Gopher]]></summary></entry></feed>