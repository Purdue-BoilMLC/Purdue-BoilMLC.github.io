<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://purdue-boilmlc.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://purdue-boilmlc.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-06-24T15:30:09+00:00</updated><id>https://purdue-boilmlc.github.io/feed.xml</id><title type="html">Boilermakers Machine Learning Collaborative (Boil-MLC)</title><subtitle>The Boilermakers Machine Learning Collaborative at Purdue University is a student-led interdisciplinary initiative established to bridge theoretical foundations with practical applications in the field of machine learning. Targeted primarily at PhD and Masters students, BoilMLC aims to leverage the diverse academic backgrounds of its members to foster innovative research and enhance the academic and professional stature of its participants. </subtitle><entry><title type="html">Understanding Wasserstein GANs: A Mathematical Perspective</title><link href="https://purdue-boilmlc.github.io/blog/2024/WGAN/" rel="alternate" type="text/html" title="Understanding Wasserstein GANs: A Mathematical Perspective"/><published>2024-06-24T13:00:00+00:00</published><updated>2024-06-24T13:00:00+00:00</updated><id>https://purdue-boilmlc.github.io/blog/2024/WGAN</id><content type="html" xml:base="https://purdue-boilmlc.github.io/blog/2024/WGAN/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Generative Adversarial Networks (GANs) <a class="citation" href="#goodfellow2014generative">(Goodfellow et al., 2014)</a> have revolutionized the field of generative modeling by enabling the creation of highly realistic images, videos, and other media. Despite their success, standard GANs frequently face significant training challenges, such as instability, mode collapse, and difficulties in achieving convergence. These issues often stem from the choice of divergence metrics used to measure the difference between real and generated data distributions.</p> <p>In this blog post, we explore how Wasserstein GANs (WGANs) <a class="citation" href="#arjovsky2017wasserstein">(Arjovsky et al., 2017)</a> address these challenges by leveraging the Wasserstein distance, leading to more stable training and superior generative performance. We delve into the mathematical foundations of WGANs, discuss their improvements over standard GANs, and examine their practical implications.</p> <h2 id="overcoming-challenges-in-standard-gans">Overcoming Challenges in Standard GANs</h2> <p>Standard GANs consist of two neural networks—the generator (G) and the discriminator (D)—engaged in a minimax game. The generator aims to create samples indistinguishable from real data, while the discriminator strives to differentiate between real and generated samples. The loss functions for the generator and discriminator can be expressed as:</p> \[\label{eq:gan_loss} \begin{aligned} \min_G \max_D &amp; \, \mathbb{E}_{x \sim P_r} [\log D(x)] + \mathbb{E}_{\hat{x} \sim P_g} [\log (1 - D(\hat{x}) )], \end{aligned}\] <p>where \(P_r\) denotes the real data distribution, and \(P_g\) is the distribution induced by the generator over the data space.</p> <h3 id="common-issues-in-standard-gan-training">Common Issues in Standard GAN Training</h3> <ol> <li><strong>Mode Collapse</strong>: The generator produces a limited diversity of samples, causing it to capture only a few modes of the real data distribution.</li> <li><strong>Training Instability</strong>: The adversarial nature of the GAN training often leads to oscillations and lack of convergence.</li> <li><strong>Vanishing Gradients</strong>: When the discriminator becomes too effective, the generator receives negligible gradient updates, hindering its improvement.</li> </ol> <p>These problems largely arise from the nature of the Jensen-Shannon (JS) divergence used in standard GANs. When the supports of \(P_r\) and \(P_g\) do not overlap, the JS divergence becomes constant, leading to vanishing gradients.</p> <h2 id="the-wasserstein-distance">The Wasserstein Distance</h2> <p>Wasserstein GANs propose an elegant solution by replacing the JS divergence with the Wasserstein distance (also known as the Earth Mover’s distance), which measures the cost of transforming one distribution into another. This metric remains meaningful even when the distributions have non-overlapping supports.</p> <h3 id="mathematical-formulation">Mathematical Formulation</h3> <p>The Wasserstein distance between two probability distributions \(P_r\) and \(P_g\) is defined as:</p> \[\label{eq:wasserstein} W(P_r, P_g) = \inf_{\gamma \in \Pi(P_r, P_g)} \mathbb{E}_{(x, y) \sim \gamma}[\|x - y\|],\] <p>where \(\Pi(P_r, P_g)\) is the set of all joint distributions \(\gamma(x, y)\) whose marginals are \(P_r\) and \(P_g\), respectively. Intuitively, it represents the minimum amount of “work” needed to transform \(P_g\) into \(P_r\), considering the cost of moving mass from point \(x\) to \(y\).</p> <h3 id="kantorovich-rubinstein-duality">Kantorovich-Rubinstein Duality</h3> <p>In practice, directly computing the Wasserstein distance is intractable. Instead, the problem can be reformulated using Kantorovich-Rubinstein duality <a class="citation" href="#villani2009optimal">(Villani &amp; others, 2009)</a>:</p> \[\label{eq:kantorovich} W(P_r, P_g) = \sup_{\|f\|_L \leq 1} \mathbb{E}_{x \sim P_r}[f(x)] - \mathbb{E}_{x \sim P_g}[f(x)],\] <p>where the supremum is over all 1-Lipschitz functions \(f\).</p> <p>In the context of WGANs, the discriminator \(D\) is used to approximate the optimal 1-Lipschitz function \(f\). Thus, the loss functions for the generator and discriminator can be written as:</p> \[\label{eq:wgan_loss} \begin{aligned} \max_D &amp; \, \mathbb{E}_{x \sim P_r}[D(x)] - \mathbb{E}_{\hat{x} \sim P_g}[D(\hat{x})], \\ \min_G &amp; \, \mathbb{E}_{\hat{x} \sim P_g}[D(\hat{x})]. \end{aligned}\] <h3 id="enforcing-the-1-lipschitz-constraint">Enforcing the 1-Lipschitz Constraint</h3> <p>A critical requirement for the discriminator in WGANs is the 1-Lipschitz constraint. In the original WGAN formulation, this constraint was enforced by clipping the weights of the discriminator to a narrow range. However, this clipping can lead to several practical problems, such as underfitting.</p> <p>An improvement to this approach is the gradient penalty method <a class="citation" href="#gulrajani2017improved">(Gulrajani et al., 2017)</a>. Here, the 1-Lipschitz constraint is enforced by adding a penalty to the loss function, ensuring that the gradient of the discriminator with respect to its input is close to 1:</p> \[\label{eq:gradient_penalty} \mathcal{L}_{\text{GP}} = \lambda \mathbb{E}_{\hat{x} \sim P_{\hat{x}}}\left[(\|\nabla_{\hat{x}} D(\hat{x})\|_2 - 1)^2\right],\] <p>where \(P_{\hat{x}}\) is the distribution of points along straight lines between pairs of points sampled from \(P_r\) and \(P_g\).</p> <h2 id="conclusion">Conclusion</h2> <p>Wasserstein GANs mark a significant advancement in the field of generative modeling, especially for image synthesis. By addressing the fundamental shortcomings of standard GANs with the imposition of the Wasserstein distance, WGANs achieve more stable training dynamics and generate higher quality samples.</p> <p>The theoretical robustness rooted in optimal transport theory and practical improvements like gradient penalty make WGANs a critical tool in the arsenal of machine learning practitioners. As research in this area progresses, we can expect further refinements and hybrid models that continue to push the boundaries of what generative models can achieve.</p> <p>In subsequent posts, we will explore other transformative advancements in AI and machine learning, providing a comprehensive view of the state-of-the-art technologies shaping our world.</p>]]></content><author><name>Yuetian Chen</name></author><category term="take-aways"/><category term="Generative-AI"/><category term="GANs"/><summary type="html"><![CDATA[A paper review on Wasserstein GAN]]></summary></entry><entry><title type="html">Gopher: Scaling Language Models to New Heights</title><link href="https://purdue-boilmlc.github.io/blog/2024/gopher/" rel="alternate" type="text/html" title="Gopher: Scaling Language Models to New Heights"/><published>2024-05-29T13:00:00+00:00</published><updated>2024-05-29T13:00:00+00:00</updated><id>https://purdue-boilmlc.github.io/blog/2024/gopher</id><content type="html" xml:base="https://purdue-boilmlc.github.io/blog/2024/gopher/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Recent years have seen tremendous progress in the fields of natural language processing (NLP) and generative modeling. Language models like GPT-3 <a class="citation" href="#brown2020language">(Brown et al., 2020)</a> and PaLM <a class="citation" href="#chowdhery2022palm">(Chowdhery et al., 2022)</a> have demonstrated remarkable capabilities in understanding and generating human-like text. In this blog post, we delve into the groundbreaking work on the Gopher model <a class="citation" href="#rae2022scaling">(<i>Scaling Language Models: Methods, Analysis &amp; Insights from Training Gopher</i>, 2022)</a> developed by DeepMind. We will explore the architecture, key insights from this work, and its implications for the future of language models.</p> <h2 id="scaling-language-models">Scaling Language Models</h2> <p>Language models form the backbone of modern NLP systems. By learning from vast corpora of text data, these models acquire a broad understanding of language that can be applied to diverse downstream tasks. The Gopher model <a class="citation" href="#rae2022scaling">(<i>Scaling Language Models: Methods, Analysis &amp; Insights from Training Gopher</i>, 2022)</a>, developed by DeepMind, represents an important milestone in the scaling of language models.</p> <h3 id="the-gopher-architecture">The Gopher Architecture</h3> <p>Gopher is a large-scale transformer-based language model. It follows the decoder-only architecture that has become standard for models like GPT-3. However, the Gopher architecture incorporates several notable and innovative modifications which distinguish it from its predecessors:</p> <ol> <li> <p><strong>Scaling in Size</strong>: Gopher ranges up to 280 billion parameters, significantly larger than many previous models. This increase in scale aims to leverage more data and model capacity to improve performance across diverse tasks.</p> </li> <li> <p><strong>RMSNorm instead of LayerNorm</strong>: Gopher employs RMSNorm <a class="citation" href="#zhang2019root">(Zhang &amp; Sennrich, 2019)</a> instead of the more commonly used LayerNorm. RMSNorm (Root Mean Square Normalization) is computationally simpler and may lead to more stable gradients, enhancing the performance of deep networks. Mathematically, RMSNorm normalizes the inputs based on their root mean square, promoting better normalization properties by focusing on the scale of the neurons’ activations.</p> \[\text{RMSNorm}(\mathbf{x}) = \frac{\mathbf{x}}{\text{RMS}(\mathbf{x})}\] <p>where \(\text{RMS}(\mathbf{x}) = \sqrt{\frac{1}{d} \sum_{i=1}^{d} x_i^2}\), and \(d\) is the dimension of the input vector \(\mathbf{x}\).</p> </li> <li> <p><strong>Relative Positional Encodings</strong>: Gopher utilizes relative positional encodings instead of absolute positional encodings. This method allows the model to better capture the relative distances and dependencies between tokens within the input sequence. Relative positional encodings maintain consistent performance regardless of the sequence length.</p> </li> </ol> <h3 id="key-insights-from-gopher">Key Insights from Gopher</h3> <p>The Gopher paper offers several valuable insights into the behavior and benefits of scaling up language models:</p> <ol> <li> <p><strong>Task Performance</strong>: The benefits of scale are most pronounced on tasks such as reading comprehension, fact-checking, and toxicity detection. The improvements in logical and mathematical reasoning are more modest. This discrepancy can be attributed to the nature and distribution of training data, highlighting the importance of dataset curation for developing balanced and effective language models.</p> </li> <li> <p><strong>Generalization</strong>: Larger models like Gopher generalize better across a diverse set of tasks. They demonstrate enhanced zero-shot and few-shot learning capabilities, where the model has to perform tasks with minimal or no task-specific training data.</p> </li> <li> <p><strong>Ethical Considerations</strong>: As models scale, ethical and societal implications become more significant. The Gopher paper emphasizes the need for comprehensive evaluations of model biases and the importance of developing frameworks for responsible AI.</p> </li> </ol> <p>The Gopher model underscores the immense potential of scaling up language models in terms of parameter size and data volume. It points to a future where such models can serve as flexible, general-purpose tools for a wide range of applications.</p> <h2 id="conclusion">Conclusion</h2> <p>Gopher represents a significant advancement in the field of language modeling, pushing the boundaries of what large-scale models can achieve. Its novel architectural choices and insightful analysis provide a valuable blueprint for future research and development in NLP. As we continue to scale up models and refine our training techniques, we will unlock new frontiers in machine intelligence, transforming the capabilities of AI systems across various domains. However, it is crucial to address the ethical and societal implications of these powerful technologies, ensuring their development and deployment benefit all of humanity.</p>]]></content><author><name>Yuetian Chen</name></author><category term="take-aways"/><category term="NLP"/><category term="Transformer"/><category term="LLM"/><summary type="html"><![CDATA[A paper review on Scaling Language Models: Methods, Analysis & Insights from Training Gopher]]></summary></entry></feed>