<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://purdue-boilmlc.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://purdue-boilmlc.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-06-18T15:24:04+00:00</updated><id>https://purdue-boilmlc.github.io/feed.xml</id><title type="html">Boilermakers Machine Learning Collaborative (Boil-MLC)</title><subtitle>The Boilermakers Machine Learning Collaborative at Purdue University is a student-led interdisciplinary initiative established to bridge theoretical foundations with practical applications in the field of machine learning. Targeted primarily at PhD and Masters students, BoilMLC aims to leverage the diverse academic backgrounds of its members to foster innovative research and enhance the academic and professional stature of its participants. </subtitle><entry><title type="html">Advancements in Language Models and Generative Adversarial Networks</title><link href="https://purdue-boilmlc.github.io/blog/2024/take-aways/" rel="alternate" type="text/html" title="Advancements in Language Models and Generative Adversarial Networks"/><published>2024-05-29T13:00:00+00:00</published><updated>2024-05-29T13:00:00+00:00</updated><id>https://purdue-boilmlc.github.io/blog/2024/take-aways</id><content type="html" xml:base="https://purdue-boilmlc.github.io/blog/2024/take-aways/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Recent years have seen tremendous progress in the fields of natural language processing (NLP) and generative modeling. Language models like GPT-3 <a class="citation" href="#brown2020language">(Brown et al., 2020)</a> and PaLM <a class="citation" href="#chowdhery2022palm">(Chowdhery et al., 2022)</a> have demonstrated remarkable capabilities in understanding and generating human-like text. Meanwhile, generative adversarial networks (GANs) have enabled the creation of strikingly realistic images, videos, and other media.</p> <p>In this blog post, we dive into two significant papers that have advanced these domains: <span id="rae2022scaling"><i>Scaling Language Models: Methods, Analysis &amp; Insights from Training Gopher</i>. (2022).</span> and <span id="arjovsky2017wasserstein">Arjovsky, M., Chintala, S., &amp; Bottou, L. (2017). <i>Wasserstein GAN</i>.</span>. We discuss the key ideas, methodologies, and implications of these works.</p> <h2 id="scaling-language-models">Scaling Language Models</h2> <p>Language models form the backbone of modern NLP systems. By learning from vast corpora of text data, these models acquire a broad understanding of language that can be applied to diverse downstream tasks. The Gopher model <a class="citation" href="#rae2022scaling">(<i>Scaling Language Models: Methods, Analysis &amp; Insights from Training Gopher</i>, 2022)</a>, developed by DeepMind, represents an important milestone in the scaling of language models.</p> <h3 id="the-gopher-architecture">The Gopher Architecture</h3> <p>Gopher is a large-scale transformer-based language model. It follows the decoder-only architecture that has become standard for models like GPT-3. However, the Gopher architecture incorporates a few notable modifications:</p> <ol> <li> <p><strong>RMSNorm instead of LayerNorm</strong>: Gopher uses RMSNorm <a class="citation" href="#zhang2019root">(Zhang &amp; Sennrich, 2019)</a> in place of the more common LayerNorm. RMSNorm is computationally simpler and may promote more stable gradients in deep networks.</p> </li> <li> <p><strong>Relative Positional Encodings</strong>: Instead of absolute positional encodings, Gopher employs relative encodings. This allows the model to better capture the relative distances between tokens in the input sequence.</p> </li> </ol> <h3 id="key-insights-from-gopher">Key Insights from Gopher</h3> <p>The Gopher paper provides several valuable insights into the behavior of large language models:</p> <blockquote><p>The benefits of scale are most pronounced on tasks like reading comprehension, fact-checking, and toxicity detection. Logical and mathematical reasoning see more modest improvements.</p><cite><a class="citation" href="#rae2022scaling">(<i>Scaling Language Models: Methods, Analysis &amp; Insights from Training Gopher</i>, 2022)</a></cite></blockquote> <p>This discrepancy may be attributed to the distribution of the training data, underscoring the importance of dataset curation.</p> <p>Overall, Gopher demonstrates the immense potential of scaling up language models in terms of both parameter count and data size.</p> <h2 id="wasserstein-gans">Wasserstein GANs</h2> <p>GANs <a class="citation" href="#goodfellow2014generative">(Goodfellow et al., 2014)</a> have emerged as a powerful framework for generative modeling, especially in the image domain. However, training GANs is notoriously challenging, with issues like mode collapse and unstable dynamics plaguing many variants. The Wasserstein GAN (WGAN) <a class="citation" href="#arjovsky2017wasserstein">(Arjovsky et al., 2017)</a> introduces a novel approach that mitigates these problems.</p> <h3 id="overcoming-challenges-in-standard-gans">Overcoming Challenges in Standard GANs</h3> <p>In a standard GAN, the generator and discriminator are trained in an adversarial game. The generator tries to produce samples that fool the discriminator, while the discriminator learns to distinguish real from generated data. This setup can lead to several pathologies <span id="arjovsky2017wasserstein">Arjovsky, M., Chintala, S., &amp; Bottou, L. (2017). <i>Wasserstein GAN</i>.</span>:</p> <ul> <li>If the discriminator becomes too strong, it may perfectly reject all generator samples, leading to vanishing gradients and stalled training.</li> <li>The generator may collapse onto a narrow distribution, sacrificing diversity in an effort to exploit the discriminator.</li> </ul> <p>WGAN proposes a principled solution to these challenges, rooted in the theory of optimal transport.</p> <h3 id="the-wasserstein-distance">The Wasserstein Distance</h3> <p>The Wasserstein distance, also known as the Earth Moverâ€™s distance, is a metric that quantifies the dissimilarity between two probability distributions. In the context of GANs, it measures the distance between the generated distribution $P_g$ and the real data distribution $P_r$. The Wasserstein distance is defined as:</p> \[\label{eq:wasserstein} W(P_r, P_g) = \inf_{\gamma \in \Pi(P_r, P_g)} \mathbb{E}_{(x, y) \sim \gamma}[\|x - y\|]\] <p>where $\Pi(P_r, P_g)$ denotes the set of all joint distributions $\gamma(x, y)$ whose marginals are $P_r$ and $P_g$, respectively. In other words, the Wasserstein distance is the minimum cost of transporting mass from $P_r$ to $P_g$, where the cost is given by the expectation of the Euclidean distance between points.</p> <p>The key advantage of the Wasserstein distance is that it is continuous and differentiable almost everywhere, even when the supports of $P_r$ and $P_g$ do not overlap. This is in contrast to other commonly used metrics like the Jensen-Shannon divergence, which can lead to vanishing gradients in such scenarios <a class="citation" href="#arjovsky2017wasserstein">(Arjovsky et al., 2017)</a>.</p> <p>To make the Wasserstein distance tractable to compute, the Kantorovich-Rubinstein duality <a class="citation" href="#villani2009optimal">(Villani &amp; others, 2009)</a> is employed:</p> \[\label{eq:kantorovich} W(P_r, P_g) = \sup_{\|f\|_L \leq 1} \mathbb{E}_{x \sim P_r}[f(x)] - \mathbb{E}_{x \sim P_g}[f(x)]\] <p>where the supremum is taken over all 1-Lipschitz functions $f$. In practice, the discriminator in WGAN is trained to approximate the optimal $f$, while the generator is trained to minimize the Wasserstein distance.</p> <p>The Lipschitz constraint on the discriminator is enforced through gradient penalty <a class="citation" href="#gulrajani2017improved">(Gulrajani et al., 2017)</a>:</p> \[\label{eq:gradient_penalty} \mathcal{L}_{\text{GP}} = \lambda \mathbb{E}_{\hat{x} \sim P_{\hat{x}}}[(\|\nabla_{\hat{x}} D(\hat{x})\|_2 - 1)^2]\] <p>where $P_{\hat{x}}$ is the distribution of points along straight lines between pairs of points sampled from $P_r$ and $P_g$. This gradient penalty term is added to the discriminator loss to encourage 1-Lipschitzness.</p> <p>By minimizing the Wasserstein distance through this dual formulation, WGAN achieves more stable training and generates higher quality samples compared to standard GANs. The theoretical grounding in optimal transport theory provides a principled foundation for the WGAN framework.</p> <h2 id="conclusion">Conclusion</h2> <p>The Gopher and WGAN papers exemplify the rapid progress happening in language modeling and generative AI. As we scale up our models and refine our training objectives, we unlock new frontiers in machine intelligence. Gopher points towards a future where language models serve as flexible, general-purpose tools for a wide range of applications. WGAN, meanwhile, brings us closer to generating high-fidelity content with deep neural networks.</p> <p>Moving forward, it will be exciting to see how these ideas are extended and combined. The union of powerful language models and advanced generative techniques could lead to transformative breakthroughs, reshaping how we interact with AI systems. As always, however, it is crucial that we approach these developments thoughtfully, considering the broader societal implications alongside the technical innovations.</p>]]></content><author><name>Yuetian Chen</name></author><category term="take-aways"/><category term="LLM"/><category term="GAN"/><category term="Gopher"/><category term="WGAN"/><summary type="html"><![CDATA[A paper review on WGAN & Gopher]]></summary></entry></feed>